{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    884\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \"\"\"\n\u001b[0;32m--> 470\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e8e5dbc68c6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#pip install selenium\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#pip install progressbar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdestinataire\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Veuillez entrer l'adresse mail du destinataire :\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Entrez votre mot de passe SMTP :'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0madresse_envoi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pzim@free.fr'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m    841\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m         )\n\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    888\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "    #!/usr/bin/env python\n",
    "    # coding: utf-8\n",
    "\n",
    "    # J'utilise un driver Chrome 77 (ChromeDriver 77.0.3865.40) (https://chromedriver.chromium.org/downloads ). Il faut le mettre dans le même dossier que le fichier Python (emplacement_driver_chrome) et je suis sous Linux 64 bit Ubuntu.\n",
    "    # \n",
    "    # Le navigateur (Version 77.0.3865.90 (Build officiel) (64 bits)) a les options par défaut.\n",
    "    # \n",
    "    # Par ailleurs, vérifier que le swapfile est bien à 0B used à la fin de l'execution du code:\n",
    "    # sudo swapon --show pour verifier la taille du swap\n",
    "\n",
    "    # In[1]:\n",
    "#from getpass import getpass\n",
    "\n",
    "    #pip install progressbar2\n",
    "    #pip install selenium\n",
    "    #pip install progressbar\n",
    "destinataire=''\n",
    "password=''\n",
    "adresse_envoi=''\n",
    "smtp='smtp.'\n",
    "port=587\n",
    "login=adresse_envoi\n",
    "\n",
    "    # In[ ]:\n",
    "\n",
    "\n",
    "    #import requests\n",
    "    #from bs4 import BeautifulSoup\n",
    "    #from selenium.webdriver.common.keys import Keys\n",
    "    #import csv\n",
    "    #import progressbar\n",
    "try:\n",
    "        \n",
    "        import pandas as pd\n",
    "        from datetime import datetime\n",
    "        import time\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        from datetime import datetime\n",
    "        import pytz\n",
    "        from tqdm import tqdm\n",
    "        import os\n",
    "        from tqdm import tqdm\n",
    "        from email.mime.multipart import MIMEMultipart\n",
    "        from email.mime.text import MIMEText\n",
    "        import smtplib\n",
    "        import re\n",
    "        import shutil\n",
    "\n",
    "\n",
    "        # In[3]:\n",
    "\n",
    "\n",
    "        emplacement_ancien_csv='old_csv'\n",
    "        emplacement_driver_chrome='chromedriver'\n",
    "        emplacement_fichier='cases_list'\n",
    "        emplacement_fichier_affaires='cases'\n",
    "\n",
    "\n",
    "\n",
    "        # I. Liste des affaires\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        today= str(datetime.now(pytz.timezone(\"Europe/Paris\")).strftime(\"%H:%M:%S LE %d/%m/%Y\"))\n",
    "        print('\\nDEBUT DU PROGRAMME A '+today)\n",
    "\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        print('\\nI. ANALYSE DE LA LISTE DES AFFAIRES (SOMMAIRE) : \\n')\n",
    "\n",
    "\n",
    "        # a) Scrapping\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        print('  1- Lancement du scraping :\\n')\n",
    "\n",
    "\n",
    "        # In[4]:\n",
    "\n",
    "\n",
    "        options = Options()\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        options.headless = True\n",
    "        options.add_argument(\"--window-size=1920,1200\")\n",
    "        #browser.save_screenshot('screenshot.png')\n",
    "\n",
    "\n",
    "        # In[5]:\n",
    "\n",
    "\n",
    "        prefs = {\n",
    "            'profile.managed_default_content_settings.images': 2,\n",
    "            'download.prompt_for_download': False,\n",
    "            'download.directory_upgrade': True,\n",
    "            'safebrowsing.enabled': True\n",
    "        }\n",
    "        options.add_experimental_option('prefs', prefs)\n",
    "\n",
    "\n",
    "        # In[6]:\n",
    "\n",
    "\n",
    "        browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "        browser.implicitly_wait(30)\n",
    "        browser.get(\"https://icsid.worldbank.org/en/Pages/cases/AdvancedSearch.aspx\")\n",
    "        time.sleep (15)\n",
    "\n",
    "\n",
    "        # In[7]:\n",
    "\n",
    "\n",
    "        #requete = driver.get(\"https://icsid.worldbank.org/en/Pages/cases/AdvancedSearch.aspx\")\n",
    "        #time.sleep (5)\n",
    "        #page = driver.content\n",
    "        #browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[2]/div[4]/div[1]/select\").click()\n",
    "        #time.sleep (7)\n",
    "        #browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[2]/div[4]/div[1]/select\").click()\n",
    "        #aa=browser.find_element_by_xpath(\"//select[@class='CVpagecount ng-pristine ng-valid ng-touched']/option[text()='All']\").click()\n",
    "        browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[2]/div[4]/div[1]/select/option[3]\").click()\n",
    "        time.sleep (1)\n",
    "        browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[2]/div[4]/div[1]/select/option[3]\").click()\n",
    "        time.sleep (1)\n",
    "\n",
    "\n",
    "        # In[8]:\n",
    "\n",
    "\n",
    "        liste=[]\n",
    "        for a in tqdm(browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[2]/div[4]/table/tbody/tr/td[1]/a\")):\n",
    "            liste.append(str(a.get_attribute('href')))\n",
    "        #browser.close()\n",
    "        #liste                   \n",
    "\n",
    "\n",
    "        # In[9]:\n",
    "\n",
    "\n",
    "        browser.quit()\n",
    "        try: \n",
    "            browser.close()\n",
    "        except : #\"InvalidSessionIdException\"\n",
    "            pass\n",
    "\n",
    "\n",
    "        # In[10]:\n",
    "\n",
    "\n",
    "        #soup = BeautifulSoup(browser.page_source)\n",
    "\n",
    "\n",
    "        # In[11]:\n",
    "\n",
    "\n",
    "        #liste=[]\n",
    "        #for a in soup.find_all('td',{\"class\":\"casecol1\"}):\n",
    "        #  liste.append(str(a))\n",
    "        #browser.close()\n",
    "\n",
    "\n",
    "        # In[12]:\n",
    "\n",
    "\n",
    "        #liste\n",
    "\n",
    "\n",
    "        # b) Nettoyage\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        print(\"\\n  2- Nettoyage de la liste des affaires :\")\n",
    "\n",
    "\n",
    "        # In[13]:\n",
    "\n",
    "\n",
    "        #listenettoyee=[]\n",
    "        #for string in liste:\n",
    "        #    pos1=string.find('href=\"..')\n",
    "        #    pos1=pos1+len('href=\"..')\n",
    "        #    pos2=string.find('\" target=\"_blank\">')\n",
    "        #    string=string[pos1:pos2]\n",
    "        #    listenettoyee.append('https:/'+string)\n",
    "        listenettoyee=liste\n",
    "\n",
    "\n",
    "        # In[14]:\n",
    "\n",
    "\n",
    "        #listenettoyee\n",
    "\n",
    "\n",
    "        # In[15]:\n",
    "\n",
    "\n",
    "        today = str(datetime.now(pytz.timezone(\"Europe/Paris\")).strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "\n",
    "        # In[16]:\n",
    "\n",
    "\n",
    "        df = pd.DataFrame(listenettoyee)\n",
    "        df.to_csv(emplacement_fichier+'/liste_des_affaires_'+today+'.csv', index=False)\n",
    "\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        print(\"\\n Patienter 30 secondes\\n\")\n",
    "\n",
    "\n",
    "        # In[17]:\n",
    "\n",
    "\n",
    "        for i in tqdm(range(100)):\n",
    "            time.sleep(0.30)\n",
    "\n",
    "\n",
    "        # Verifications s'il y a des modifications dans la liste des affaires\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        print(\"\\n  3- Comparaison de la liste des affaires :\\n\")\n",
    "\n",
    "\n",
    "        # c) Comparaison des listes d'affaires\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        #print('COMPARAISON DES AFFAIRES\\n')\n",
    "\n",
    "\n",
    "        # In[18]:\n",
    "\n",
    "\n",
    "        dossier= os.listdir(emplacement_fichier)\n",
    "        listetecsv=[]\n",
    "        for csv in dossier:\n",
    "          listetecsv.append(csv)\n",
    "\n",
    "\n",
    "        # In[19]:\n",
    "\n",
    "\n",
    "        listetecsv.sort()\n",
    "\n",
    "\n",
    "        # Attention : Il faut au moins deux CSV. Redemarrer le code si nécessaire.\n",
    "\n",
    "        # In[20]:\n",
    "\n",
    "\n",
    "        try:\n",
    "            avant_dernier_CSV=pd.read_csv(emplacement_fichier+'/'+listetecsv[-2], error_bad_lines=False)\n",
    "            dernier_CSV=pd.read_csv(emplacement_fichier+'/'+listetecsv[-1])\n",
    "        except :\n",
    "            print(\"\\n\\n\\nAttention ! : Il faut au moins deux CSV. Relancer le programme!\\n\\n\\n\")\n",
    "\n",
    "\n",
    "        # In[21]:\n",
    "\n",
    "\n",
    "        def Diff(li1, li2): \n",
    "            li_dif = [i for i in li1 + li2 if i not in li1 or i not in li2] \n",
    "            return li_dif \n",
    "\n",
    "\n",
    "        # In[22]:\n",
    "\n",
    "\n",
    "        #adresse_modification=???????????????\n",
    "\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        #print(\"\\n4- Vérification si des modifications ont été apportées à la liste des affaires : \\n\")\n",
    "\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        print(\"\\n  4- Création du message si des modifications ont été apportées \\à la liste des affaires dans le sommaire:\\n\")\n",
    "\n",
    "\n",
    "        # In[23]:\n",
    "\n",
    "\n",
    "        today = str(datetime.now(pytz.timezone(\"Europe/Paris\")).strftime(\"%d/%m/%Y à %H:%M:%S\"))\n",
    "        message1=[]\n",
    "        if avant_dernier_CSV.equals(dernier_CSV)==False:\n",
    "          print ('\\nModifications apportées au sommaire')\n",
    "          avant_dernier_CSVliste=avant_dernier_CSV.values.tolist()\n",
    "          dernier_CSVliste=dernier_CSV.values.tolist()\n",
    "          modificationsliste=Diff(dernier_CSVliste,avant_dernier_CSVliste)\n",
    "          modificationsliste=['\\n'.join(sub_list) for sub_list in modificationsliste]\n",
    "\n",
    "\n",
    "          for i in tqdm(range(0,int(len(modificationsliste)))):\n",
    "\n",
    "            chaine = str(modificationsliste[i])\n",
    "            pos1= modificationsliste[i].find('CaseNo=')\n",
    "            pos1= pos1+len('CaseNo=')\n",
    "            sousChaine=chaine[pos1:]\n",
    "            sousChaine=sousChaine.replace('%20',' ')\n",
    "\n",
    "            modificationsliste[i]=modificationsliste[i].replace(' ','%20')\n",
    "            message1.append(\"* Voici l'affaire modifiée, supprimée ou ajoutée, dans le sommaire de la page https://icsid.worldbank.org/en/Pages/cases/AdvancedSearch.aspx :\\nNom de l'affaire : \"+str(sousChaine)+\" \\nAdresse de l'affaire : \"+modificationsliste[i]+\" \\nNofication créée le \"+today)\n",
    "            message1=list(set(message1))\n",
    "           #print(message1)\n",
    "          #sauvegarde message\n",
    "            i=i+1\n",
    "          #message1.append(str(liste_mail)+' Nofication crée le '+today)\n",
    "            print('\\nModifications sauvegardées dans la liste message1')\n",
    "        else :\n",
    "          print('\\nPas de modification du sommaire, pas de notification créée\\n')\n",
    "          message1=[]\n",
    "\n",
    "\n",
    "        # In[24]:\n",
    "\n",
    "\n",
    "        #print(message1)\n",
    "\n",
    "\n",
    "        # II. Affaires\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        print(\"\\n\\nII. ANALYSE DE CHAQUE AFFAIRE :\\n\")\n",
    "\n",
    "\n",
    "        # a) Scraping\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        print('  1- Lancement du scraping :')\n",
    "\n",
    "\n",
    "        # In[25]:\n",
    "\n",
    "\n",
    "        listenettoyee.sort()\n",
    "        len(listenettoyee)\n",
    "\n",
    "\n",
    "        # In[26]:\n",
    "\n",
    "\n",
    "        listenettoyee2=[]\n",
    "        #for i in listenettoyee:\n",
    "        for j in range(0,len(listenettoyee)):\n",
    "            listenettoyee2.append(listenettoyee[j].replace('/', '_'))\n",
    "\n",
    "\n",
    "        # In[27]:\n",
    "\n",
    "\n",
    "        listenettoyee2.sort()\n",
    "        #len(listenettoyee2)\n",
    "\n",
    "\n",
    "        # In[28]:\n",
    "\n",
    "\n",
    "        listenettoyee11=listenettoyee[:int(len(listenettoyee)/3)]\n",
    "        #len(listenettoyee11)\n",
    "\n",
    "\n",
    "        # In[29]:\n",
    "\n",
    "\n",
    "        listenettoyee12=listenettoyee[int(len(listenettoyee)/3):int(len(listenettoyee)*2/3)]\n",
    "        #len(listenettoyee12)\n",
    "\n",
    "\n",
    "        # In[30]:\n",
    "\n",
    "\n",
    "        listenettoyee13=listenettoyee[int(len(listenettoyee)*2/3):]\n",
    "        #len(listenettoyee13)\n",
    "\n",
    "\n",
    "        # In[31]:\n",
    "\n",
    "\n",
    "        #len(listenettoyee11)+len(listenettoyee12)+len(listenettoyee13)\n",
    "\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        print('\\n    1.1- Scraping 1/3 : \\nSCRAPING DES AFFAIRES DE 1 A '+str(len(listenettoyee11))+' SUR '+str(len(listenettoyee2))+' :\\n')\n",
    "\n",
    "\n",
    "        # In[32]:\n",
    "\n",
    "\n",
    "        today2 = str(datetime.now(pytz.timezone(\"Europe/Paris\")).strftime(\"%Y%m%d_%H%M%S\"))\n",
    "        options = Options()\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.headless = True\n",
    "        options.add_argument(\"--window-size=1920,1200\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        prefs = {\n",
    "            'profile.managed_default_content_settings.images': 2,\n",
    "            'download.prompt_for_download': False,\n",
    "            'download.directory_upgrade': True,\n",
    "            'safebrowsing.enabled': True\n",
    "        }\n",
    "        options.add_experimental_option('prefs', prefs)\n",
    "        #browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "        i=-1\n",
    "        for cases in tqdm(listenettoyee11) :\n",
    "            i=i+1\n",
    "            try:\n",
    "\n",
    "            #requete_affaires = requests.get(cases)\n",
    "            #page_affaires = requete_affaires.content\n",
    "            #soup_affaires = BeautifulSoup(page_affaires)\n",
    "\n",
    "                browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                globals()['liste_affaire'+cases[66:]+\"_\"+today2]=[]\n",
    "                browser.implicitly_wait(30)\n",
    "            #browser.execute_script(\"window.open('');\")\n",
    "            #Window_List = browser.window_handles\n",
    "            #browser.switch_to_window(browser.window_handles[0])\n",
    "                browser.get(str(cases))\n",
    "                time.sleep(0.1)\n",
    "                browser.get(str(cases))\n",
    "                time.sleep(0.1)\n",
    "\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "                print('\\n!!!!!!!!! ATTENTION probleme détecté, le processus recommence!!!!!!!!! Attendre 5 minutes\\n')\n",
    "                browser.quit()\n",
    "                try: \n",
    "                    browser.close()\n",
    "                except :\n",
    "                    pass\n",
    "\n",
    "                time.sleep(300)\n",
    "                browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                browser.implicitly_wait(30)\n",
    "                browser.get(str(cases))\n",
    "                time.sleep(10)\n",
    "                browser.get(str(cases))\n",
    "                time.sleep(10)\n",
    "            #browser.find_element(by.linkText(str(cases))).sendKeys(Keys.chord(Keys.CONTROL,\"t\"))\n",
    "            #time.sleep (2.5)\n",
    "            #if i==200 or i==400 or i== 600 or i== 800:\n",
    "             #   print(\"sleep\")\n",
    "              #  time.sleep (60)\n",
    "            #Scrapping du titre :\n",
    "            for titre in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[1]/span\"):\n",
    "                try:\n",
    "                    globals()['liste_affaire'+cases[66:]+\"_\"+today2].append(str(titre.text))\n",
    "                    df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                    df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "                    if len(globals()['liste_affaire'+cases[66:]+\"_\"+today2])==0 or len(globals()['liste_affaire'+cases[66:]+\"_\"+today2][0])<25 or globals()['liste_affaire'+cases[66:]+\"_\"+today2][0]=='nan' or globals()['liste_affaire'+cases[66:]+\"_\"+today2][0]=='':\n",
    "                        print(\"\\nATTTENTION Le titre n'a pas été scrapé !!!, le processus recommence dans 30 secondes\\n\")\n",
    "                        #print(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                        browser.quit()\n",
    "                        try: \n",
    "                            browser.close()\n",
    "                        except :\n",
    "                            pass\n",
    "                        time.sleep(30)\n",
    "                        browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                        browser.implicitly_wait(30)\n",
    "                        browser.get(str(cases))\n",
    "                        time.sleep(0.1)\n",
    "                        browser.get(str(cases))\n",
    "                        time.sleep(10)\n",
    "                        globals()['liste_affaire'+cases[66:]+\"_\"+today2]=[]\n",
    "                        for titre in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[1]/span\"):\n",
    "                            globals()['liste_affaire'+cases[66:]+\"_\"+today2].append(str(titre.text))\n",
    "                            df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                            df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "\n",
    "                except Exception as e: \n",
    "                    print(e)\n",
    "                    print('\\n!!!!!!!!! ATTENTION probleme détecté, le processus recommence!!!!!!!!! Attendre 5 minutes\\n')\n",
    "                    browser.quit()\n",
    "                    try: \n",
    "                        browser.close()\n",
    "                    except :\n",
    "                        pass\n",
    "\n",
    "                    time.sleep(300)\n",
    "                    browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                    browser.implicitly_wait(30)\n",
    "                    browser.get(str(cases))\n",
    "                    time.sleep(10)\n",
    "                    browser.get(str(cases))\n",
    "                    time.sleep(10)\n",
    "                    globals()['liste_affaire'+cases[66:]+\"_\"+today2]=[]\n",
    "                    for titre in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[1]/span\"):\n",
    "                        globals()['liste_affaire'+cases[66:]+\"_\"+today2].append(str(titre.text))\n",
    "                        df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                        df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "                        if len(globals()['liste_affaire'+cases[66:]+\"_\"+today2])==0 or len(globals()['liste_affaire'+cases[66:]+\"_\"+today2][0])<25 or globals()['liste_affaire'+cases[66:]+\"_\"+today2][0]=='nan' or globals()['liste_affaire'+cases[66:]+\"_\"+today2][0]=='':\n",
    "                            print(\"\\nATTTENTION Le titre n'a pas été scrapé !!!, le processus recommence dans 30 secondes\\n\")\n",
    "                            #print(globals()['liste_affaire'+cases[66:]+\"_\"+today2][0])\n",
    "                            browser.quit()\n",
    "                            try: \n",
    "                                browser.close()\n",
    "                            except :\n",
    "                                pass\n",
    "                            time.sleep(30)\n",
    "                            browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                            browser.implicitly_wait(30)\n",
    "                            browser.get(str(cases))\n",
    "                            time.sleep(0.1)\n",
    "                            browser.get(str(cases))\n",
    "                            time.sleep(10)\n",
    "                            globals()['liste_affaire'+cases[66:]+\"_\"+today2]=[]\n",
    "                            for titre in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[1]/span\"):\n",
    "                                globals()['liste_affaire'+cases[66:]+\"_\"+today2].append(str(titre.text))\n",
    "                                df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                                df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "\n",
    "            #Scraping de l'onglet principal : \n",
    "\n",
    "            for a in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[2]\"):\n",
    "            #liste.append(str(a.get_attribute('href')))\n",
    "\n",
    "           # soup_affaires = BeautifulSoup(browser.page_source)\n",
    "            #for a in soup_affaires.find_all('div',{'class':'article article-body'}):\n",
    "                try:\n",
    "                    globals()['liste_affaire'+cases[66:]+\"_\"+today2].append(str(a.text))\n",
    "                    df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                    df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "\n",
    "                    if len(globals()['liste_affaire'+cases[66:]+\"_\"+today2])<=1 or len(globals()['liste_affaire'+cases[66:]+\"_\"+today2][1])<50 or globals()['liste_affaire'+cases[66:]+\"_\"+today2][1]=='nan' or globals()['liste_affaire'+cases[66:]+\"_\"+today2][1]=='':\n",
    "                        print(\"\\nATTTENTION L'onglet principal n'a pas été scrapé !!!, le processus recommence dans 30 secondes\\n\")\n",
    "                        #print(globals()['liste_affaire'+cases[66:]+\"_\"+today2][1])\n",
    "                        globals()['liste_affaire'+cases[66:]+\"_\"+today2]=globals()['liste_affaire'+cases[66:]+\"_\"+today2][0]\n",
    "                        browser.quit()\n",
    "                        try: \n",
    "                            browser.close()\n",
    "                        except :\n",
    "                            pass\n",
    "                        time.sleep(30)\n",
    "                        browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                        browser.implicitly_wait(30)\n",
    "                        browser.get(str(cases))\n",
    "                        time.sleep(0.1)\n",
    "                        browser.get(str(cases))\n",
    "                        time.sleep(2)\n",
    "                        for a in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[2]\"):\n",
    "                            globals()['liste_affaire'+cases[66:]+\"_\"+today2].insert(1, str(a.text))\n",
    "                            df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                            df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "                except Exception as e: \n",
    "                    print(e)\n",
    "                    print('\\n!!!!!!!!! ATTENTION probleme détecté, le processus recommence!!!!!!!!! Attendre 5 minutes\\n')\n",
    "                    browser.quit()\n",
    "                    try: \n",
    "                        browser.close()\n",
    "                    except :\n",
    "                        pass\n",
    "\n",
    "                    time.sleep(300)\n",
    "                    browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                    browser.implicitly_wait(30)\n",
    "                    browser.get(str(cases))\n",
    "                    time.sleep(10)\n",
    "                    browser.get(str(cases))\n",
    "                    time.sleep(10)\n",
    "                    globals()['liste_affaire'+cases[66:]+\"_\"+today2]=globals()['liste_affaire'+cases[66:]+\"_\"+today2][0]\n",
    "                    for a in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[2]\"):\n",
    "            #liste.append(str(a.get_attribute('href')))\n",
    "\n",
    "           # soup_affaires = BeautifulSoup(browser.page_source)\n",
    "            #for a in soup_affaires.find_all('div',{'class':'article article-body'}):\n",
    "                        globals()['liste_affaire'+cases[66:]+\"_\"+today2].insert(1, str(a.text))\n",
    "                        df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                        df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "\n",
    "                    if len(globals()['liste_affaire'+cases[66:]+\"_\"+today2])<=1 or len(globals()['liste_affaire'+cases[66:]+\"_\"+today2][1])<50 or globals()['liste_affaire'+cases[66:]+\"_\"+today2][1]=='nan' or globals()['liste_affaire'+cases[66:]+\"_\"+today2][1]=='':\n",
    "                        print(\"\\nATTTENTION L'onglet principal n'a pas été scrapé !!!, le processus recommence dans 30 secondes\\n\")\n",
    "                        #print(globals()['liste_affaire'+cases[66:]+\"_\"+today2][1])\n",
    "                        browser.quit()\n",
    "                        try: \n",
    "                            browser.close()\n",
    "                        except :\n",
    "                            pass\n",
    "                        globals()['liste_affaire'+cases[66:]+\"_\"+today2]=globals()['liste_affaire'+cases[66:]+\"_\"+today2][0]\n",
    "                        browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                        browser.implicitly_wait(30)\n",
    "                        time.sleep(30)\n",
    "                        browser.get(str(cases))\n",
    "                        time.sleep(0.1)\n",
    "                        browser.get(str(cases))\n",
    "                        time.sleep(2)\n",
    "                        for a in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[2]\"):\n",
    "                            globals()['liste_affaire'+cases[66:]+\"_\"+today2].insert(1, str(a.text))\n",
    "                            df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                            df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "\n",
    "           # Scraping de l'onglet Materials :\n",
    "           # browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[2]/a\").click()\n",
    "           # browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[2]/a\").click()\n",
    "\n",
    "\n",
    "           # time.sleep(1)\n",
    "           # soup_affaires2 = BeautifulSoup(browser.page_source)\n",
    "           # for b in soup_affaires2.find_all('div',{'class':'tab-content cdtl'}):\n",
    "           #   globals()['liste_affaire'+cases[66:]+\"_\"+today2].append(str(b))\n",
    "           #   df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "           #   df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "\n",
    "\n",
    "            #Scraping de l'onglet Procedural Details :\n",
    "            try:\n",
    "                browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[3]/a\").click()\n",
    "                time.sleep(0.1)\n",
    "                browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[3]/a\").click()\n",
    "                time.sleep(1)\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "                print('\\n!!!!!!!!! ATTENTION probleme détecté, le processus recommence!!!!!!!!! Attendre 5 minutes\\n')\n",
    "                browser.quit()\n",
    "                try: \n",
    "                    browser.close()\n",
    "                except :\n",
    "                    pass\n",
    "                time.sleep(300)\n",
    "                browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                browser.implicitly_wait(30)\n",
    "                browser.get(str(cases))\n",
    "                time.sleep(10)\n",
    "                browser.get(str(cases))\n",
    "                time.sleep(10)\n",
    "                browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[3]/a\").click()\n",
    "                time.sleep(0.1)\n",
    "                browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[3]/a\").click()\n",
    "                time.sleep(1)\n",
    "\n",
    "            #soup_affaires3 = BeautifulSoup(browser.page_source)\n",
    "            for c in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[2]\"):\n",
    "            #for c in soup_affaires3.find_all('div',{'class':'tab-content cdtl'}):\n",
    "                try:\n",
    "                    globals()['liste_affaire'+cases[66:]+\"_\"+today2].append(str(c.text))\n",
    "                    df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                    df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "                    time.sleep(0.1)\n",
    "                    if len(globals()['liste_affaire'+cases[66:]+\"_\"+today2])< 3 or (str(globals()['liste_affaire'+cases[66:]+\"_\"+today2][2]) != 'No References Available' and (len(globals()['liste_affaire'+cases[66:]+\"_\"+today2])< 2 or len(globals()['liste_affaire'+cases[66:]+\"_\"+today2][2])<50 or globals()['liste_affaire'+cases[66:]+\"_\"+today2][2]=='nan' or globals()['liste_affaire'+cases[66:]+\"_\"+today2][2]=='')):\n",
    "                        #print(globals()['liste_affaire'+cases[66:]+\"_\"+today2][2])\n",
    "                        print(\"\\nL'onglet 'Procedural Details' est anormalement vide, le processus recommence le scrap dans 30 secondes\\n\")\n",
    "                        browser.quit()\n",
    "                        try: \n",
    "                            browser.close()\n",
    "                        except :\n",
    "                            pass\n",
    "                        time.sleep(30)\n",
    "                        browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                        browser.implicitly_wait(30)\n",
    "                        browser.get(str(cases))\n",
    "                        time.sleep(0.1)\n",
    "                        browser.get(str(cases))\n",
    "                        time.sleep(2)\n",
    "                        browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[3]/a\").click()\n",
    "                        time.sleep(0.1)\n",
    "                        browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[3]/a\").click()\n",
    "                        time.sleep(2)\n",
    "                        globals()['liste_affaire'+cases[66:]+\"_\"+today2]=globals()['liste_affaire'+cases[66:]+\"_\"+today2][0:2]\n",
    "                        for c in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[2]\"):\n",
    "            #for c in soup_affaires3.find_all('div',{'class':'tab-content cdtl'}):\n",
    "                            globals()['liste_affaire'+cases[66:]+\"_\"+today2].insert(2, str(c.text))\n",
    "                            df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                            df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "                            time.sleep(0.1)\n",
    "\n",
    "                except Exception as e: \n",
    "                    print(e)\n",
    "                    print('\\n!!!!!!!!! ATTENTION probleme détecté, le processus recommence!!!!!!!!! Attendre 5 minutes\\n')\n",
    "                    browser.quit()\n",
    "                    try: \n",
    "                        browser.close()\n",
    "                    except :\n",
    "                        pass\n",
    "                    time.sleep(300)\n",
    "                    browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                    browser.implicitly_wait(30)\n",
    "                    browser.get(str(cases))\n",
    "                    time.sleep(10)\n",
    "                    browser.get(str(cases))\n",
    "                    time.sleep(10)\n",
    "                    browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[3]/a\").click()\n",
    "                    time.sleep(0.1)\n",
    "                    browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[3]/a\").click()\n",
    "                    time.sleep(1)\n",
    "                    globals()['liste_affaire'+cases[66:]+\"_\"+today2]=globals()['liste_affaire'+cases[66:]+\"_\"+today2][0:2]\n",
    "                    for c in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[2]\"):\n",
    "                        globals()['liste_affaire'+cases[66:]+\"_\"+today2].append(str(c.text))\n",
    "                        df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                        df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "                        time.sleep(0.1)\n",
    "                        if len(globals()['liste_affaire'+cases[66:]+\"_\"+today2])<= 2 or (str(globals()['liste_affaire'+cases[66:]+\"_\"+today2][2]) != 'No References Available' and (len(globals()['liste_affaire'+cases[66:]+\"_\"+today2])< 2 or len(globals()['liste_affaire'+cases[66:]+\"_\"+today2][2])<50 or globals()['liste_affaire'+cases[66:]+\"_\"+today2][2]=='nan' or globals()['liste_affaire'+cases[66:]+\"_\"+today2][2]=='')):\n",
    "                            #print(globals()['liste_affaire'+cases[66:]+\"_\"+today2][2])\n",
    "                            print(\"\\nL'onglet 'Procedural Details' est anormalement vide, le processus recommence le scrap dans 30 secondes\\n\")\n",
    "                            browser.quit()\n",
    "                            try: \n",
    "                                browser.close()\n",
    "                            except :\n",
    "                                pass\n",
    "                            browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                            browser.implicitly_wait(30)\n",
    "                            time.sleep(30)\n",
    "                            browser.get(str(cases))\n",
    "                            time.sleep(0.1)\n",
    "                            browser.get(str(cases))\n",
    "                            time.sleep(2)\n",
    "                            browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[3]/a\").click()\n",
    "                            time.sleep(0.1)\n",
    "                            browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[3]/a\").click()\n",
    "                            time.sleep(2)\n",
    "                            globals()['liste_affaire'+cases[66:]+\"_\"+today2]=globals()['liste_affaire'+cases[66:]+\"_\"+today2][0:2]\n",
    "                            for c in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[2]\"):\n",
    "            #for c in soup_affaires3.find_all('div',{'class':'tab-content cdtl'}):\n",
    "                                globals()['liste_affaire'+cases[66:]+\"_\"+today2].append(str(c.text))\n",
    "                                df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                                df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "                                time.sleep(0.1)\n",
    "\n",
    "\n",
    "\n",
    "            browser.close()\n",
    "\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        print(\"\\nSCRAPING 1/3 REUSSI !\")\n",
    "\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        browser.quit()\n",
    "        try: \n",
    "            browser.close()\n",
    "        except :\n",
    "            pass\n",
    "\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        print(\"\\n Patienter 2 minutes\\n\")\n",
    "\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        for i in tqdm(range(100)):\n",
    "            time.sleep(1.20)\n",
    "\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        print('\\n\\n    1.2- Scraping 2/3 : \\nSCRAPING DES AFFAIRES DE '+str(len(listenettoyee11))+' A '+str((len(listenettoyee11)+len(listenettoyee12)))+' SUR '+str(len(listenettoyee2))+' :\\n')\n",
    "\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        options = Options()\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.headless = True\n",
    "        options.add_argument(\"--window-size=1920,1200\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        prefs = {\n",
    "            'profile.managed_default_content_settings.images': 2,\n",
    "            'download.prompt_for_download': False,\n",
    "            'download.directory_upgrade': True,\n",
    "            'safebrowsing.enabled': True\n",
    "        }\n",
    "        options.add_experimental_option('prefs', prefs)\n",
    "        i=len(listenettoyee11)-1\n",
    "        for cases in tqdm(listenettoyee12):\n",
    "            i=i+1\n",
    "            try:\n",
    "\n",
    "            #requete_affaires = requests.get(cases)\n",
    "            #page_affaires = requete_affaires.content\n",
    "            #soup_affaires = BeautifulSoup(page_affaires)\n",
    "\n",
    "                browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                globals()['liste_affaire'+cases[66:]+\"_\"+today2]=[]\n",
    "                browser.implicitly_wait(30)\n",
    "            #browser.execute_script(\"window.open('');\")\n",
    "            #Window_List = browser.window_handles\n",
    "            #browser.switch_to_window(browser.window_handles[0])\n",
    "                browser.get(str(cases))\n",
    "                time.sleep(0.1)\n",
    "                browser.get(str(cases))\n",
    "                time.sleep(0.1)\n",
    "\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "                print('\\n!!!!!!!!! ATTENTION probleme détecté, le processus recommence!!!!!!!!! Attendre 5 minutes\\n')\n",
    "                browser.quit()\n",
    "                try: \n",
    "                    browser.close()\n",
    "                except :\n",
    "                    pass\n",
    "\n",
    "                time.sleep(300)\n",
    "                browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                browser.implicitly_wait(30)\n",
    "                browser.get(str(cases))\n",
    "                time.sleep(10)\n",
    "                browser.get(str(cases))\n",
    "                time.sleep(10)\n",
    "\n",
    "\n",
    "            #browser.find_element(by.linkText(str(cases))).sendKeys(Keys.chord(Keys.CONTROL,\"t\"))\n",
    "            #time.sleep (2.5)\n",
    "            #if i==200 or i==400 or i== 600 or i== 800:\n",
    "             #   print(\"sleep\")\n",
    "              #  time.sleep (60)\n",
    "            #Scrapping du titre :\n",
    "            for titre in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[1]/span\"):\n",
    "                try:\n",
    "                    globals()['liste_affaire'+cases[66:]+\"_\"+today2].append(str(titre.text))\n",
    "                    df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                    df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "                    if len(globals()['liste_affaire'+cases[66:]+\"_\"+today2])==0 or len(globals()['liste_affaire'+cases[66:]+\"_\"+today2][0])<25 or globals()['liste_affaire'+cases[66:]+\"_\"+today2][0]=='nan' or globals()['liste_affaire'+cases[66:]+\"_\"+today2][0]=='':\n",
    "                        print(\"\\nATTTENTION Le titre n'a pas été scrapé !!!, le processus recommence dans 30 secondes\\n\")\n",
    "                        #print(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                        browser.quit()\n",
    "                        try: \n",
    "                            browser.close()\n",
    "                        except :\n",
    "                            pass\n",
    "                        time.sleep(30)\n",
    "                        browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                        browser.implicitly_wait(30)\n",
    "                        browser.get(str(cases))\n",
    "                        time.sleep(0.1)\n",
    "                        browser.get(str(cases))\n",
    "                        time.sleep(10)\n",
    "                        globals()['liste_affaire'+cases[66:]+\"_\"+today2]=[]\n",
    "                        for titre in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[1]/span\"):\n",
    "                            globals()['liste_affaire'+cases[66:]+\"_\"+today2].append(str(titre.text))\n",
    "                            df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                            df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "\n",
    "                except Exception as e: \n",
    "                    print(e)\n",
    "                    print('\\n!!!!!!!!! ATTENTION probleme détecté, le processus recommence!!!!!!!!! Attendre 5 minutes\\n')\n",
    "                    browser.quit()\n",
    "                    try: \n",
    "                        browser.close()\n",
    "                    except :\n",
    "                        pass\n",
    "\n",
    "                    time.sleep(300)\n",
    "                    browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                    browser.implicitly_wait(30)\n",
    "                    browser.get(str(cases))\n",
    "                    time.sleep(10)\n",
    "                    browser.get(str(cases))\n",
    "                    time.sleep(10)\n",
    "                    globals()['liste_affaire'+cases[66:]+\"_\"+today2]=[]\n",
    "                    for titre in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[1]/span\"):\n",
    "                        globals()['liste_affaire'+cases[66:]+\"_\"+today2].append(str(titre.text))\n",
    "                        df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                        df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "                        if len(globals()['liste_affaire'+cases[66:]+\"_\"+today2])==0 or len(globals()['liste_affaire'+cases[66:]+\"_\"+today2][0])<25 or globals()['liste_affaire'+cases[66:]+\"_\"+today2][0]=='nan' or globals()['liste_affaire'+cases[66:]+\"_\"+today2][0]=='':\n",
    "                            print(\"\\nATTTENTION Le titre n'a pas été scrapé !!!, le processus recommence dans 30 secondes\\n\")\n",
    "                            #print(globals()['liste_affaire'+cases[66:]+\"_\"+today2][0])\n",
    "                            browser.quit()\n",
    "                            try: \n",
    "                                browser.close()\n",
    "                            except :\n",
    "                                pass\n",
    "                            time.sleep(30)\n",
    "                            browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                            browser.implicitly_wait(30)\n",
    "                            browser.get(str(cases))\n",
    "                            time.sleep(0.1)\n",
    "                            browser.get(str(cases))\n",
    "                            time.sleep(10)\n",
    "                            globals()['liste_affaire'+cases[66:]+\"_\"+today2]=[]\n",
    "                            for titre in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[1]/span\"):\n",
    "                                globals()['liste_affaire'+cases[66:]+\"_\"+today2].append(str(titre.text))\n",
    "                                df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                                df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "\n",
    "            #Scraping de l'onglet principal : \n",
    "\n",
    "            for a in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[2]\"):\n",
    "            #liste.append(str(a.get_attribute('href')))\n",
    "\n",
    "           # soup_affaires = BeautifulSoup(browser.page_source)\n",
    "            #for a in soup_affaires.find_all('div',{'class':'article article-body'}):\n",
    "                try:\n",
    "                    globals()['liste_affaire'+cases[66:]+\"_\"+today2].append(str(a.text))\n",
    "                    df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                    df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "\n",
    "                    if len(globals()['liste_affaire'+cases[66:]+\"_\"+today2])<=1 or len(globals()['liste_affaire'+cases[66:]+\"_\"+today2][1])<50 or globals()['liste_affaire'+cases[66:]+\"_\"+today2][1]=='nan' or globals()['liste_affaire'+cases[66:]+\"_\"+today2][1]=='':\n",
    "                        print(\"\\nATTTENTION L'onglet principal n'a pas été scrapé !!!, le processus recommence dans 30 secondes\\n\")\n",
    "                        #print(globals()['liste_affaire'+cases[66:]+\"_\"+today2][1])\n",
    "                        globals()['liste_affaire'+cases[66:]+\"_\"+today2]=globals()['liste_affaire'+cases[66:]+\"_\"+today2][0]\n",
    "                        browser.quit()\n",
    "                        try: \n",
    "                            browser.close()\n",
    "                        except :\n",
    "                            pass\n",
    "                        time.sleep(30)\n",
    "                        browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                        browser.implicitly_wait(30)\n",
    "                        browser.get(str(cases))\n",
    "                        time.sleep(0.1)\n",
    "                        browser.get(str(cases))\n",
    "                        time.sleep(2)\n",
    "                        for a in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[2]\"):\n",
    "                            globals()['liste_affaire'+cases[66:]+\"_\"+today2].insert(1, str(a.text))\n",
    "                            df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                            df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "                except Exception as e: \n",
    "                    print(e)\n",
    "                    print('\\n!!!!!!!!! ATTENTION probleme détecté, le processus recommence!!!!!!!!! Attendre 5 minutes\\n')\n",
    "                    browser.quit()\n",
    "                    try: \n",
    "                        browser.close()\n",
    "                    except :\n",
    "                        pass\n",
    "\n",
    "                    time.sleep(300)\n",
    "                    browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                    browser.implicitly_wait(30)\n",
    "                    browser.get(str(cases))\n",
    "                    time.sleep(10)\n",
    "                    browser.get(str(cases))\n",
    "                    time.sleep(10)\n",
    "                    globals()['liste_affaire'+cases[66:]+\"_\"+today2]=globals()['liste_affaire'+cases[66:]+\"_\"+today2][0]\n",
    "                    for a in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[2]\"):\n",
    "            #liste.append(str(a.get_attribute('href')))\n",
    "\n",
    "           # soup_affaires = BeautifulSoup(browser.page_source)\n",
    "            #for a in soup_affaires.find_all('div',{'class':'article article-body'}):\n",
    "                        globals()['liste_affaire'+cases[66:]+\"_\"+today2].insert(1, str(a.text))\n",
    "                        df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                        df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "\n",
    "                    if len(globals()['liste_affaire'+cases[66:]+\"_\"+today2])<=1 or len(globals()['liste_affaire'+cases[66:]+\"_\"+today2][1])<50 or globals()['liste_affaire'+cases[66:]+\"_\"+today2][1]=='nan' or globals()['liste_affaire'+cases[66:]+\"_\"+today2][1]=='':\n",
    "                        print(\"\\nATTTENTION L'onglet principal n'a pas été scrapé !!!, le processus recommence dans 30 secondes\\n\")\n",
    "                        #print(globals()['liste_affaire'+cases[66:]+\"_\"+today2][1])\n",
    "                        browser.quit()\n",
    "                        try: \n",
    "                            browser.close()\n",
    "                        except :\n",
    "                            pass\n",
    "                        globals()['liste_affaire'+cases[66:]+\"_\"+today2]=globals()['liste_affaire'+cases[66:]+\"_\"+today2][0]\n",
    "                        browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                        browser.implicitly_wait(30)\n",
    "                        time.sleep(30)\n",
    "                        browser.get(str(cases))\n",
    "                        time.sleep(0.1)\n",
    "                        browser.get(str(cases))\n",
    "                        time.sleep(2)\n",
    "                        for a in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[2]\"):\n",
    "                            globals()['liste_affaire'+cases[66:]+\"_\"+today2].insert(1, str(a.text))\n",
    "                            df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                            df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "\n",
    "           # Scraping de l'onglet Materials :\n",
    "           # browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[2]/a\").click()\n",
    "           # browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[2]/a\").click()\n",
    "\n",
    "\n",
    "           # time.sleep(1)\n",
    "           # soup_affaires2 = BeautifulSoup(browser.page_source)\n",
    "           # for b in soup_affaires2.find_all('div',{'class':'tab-content cdtl'}):\n",
    "           #   globals()['liste_affaire'+cases[66:]+\"_\"+today2].append(str(b))\n",
    "           #   df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "           #   df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "\n",
    "\n",
    "            #Scraping de l'onglet Procedural Details :\n",
    "            try:\n",
    "                browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[3]/a\").click()\n",
    "                time.sleep(0.1)\n",
    "                browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[3]/a\").click()\n",
    "                time.sleep(1)\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "                print('\\n!!!!!!!!! ATTENTION probleme détecté, le processus recommence!!!!!!!!! Attendre 5 minutes\\n')\n",
    "                browser.quit()\n",
    "                try: \n",
    "                    browser.close()\n",
    "                except :\n",
    "                    pass\n",
    "                time.sleep(300)\n",
    "                browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                browser.implicitly_wait(30)\n",
    "                browser.get(str(cases))\n",
    "                time.sleep(10)\n",
    "                browser.get(str(cases))\n",
    "                time.sleep(10)\n",
    "                browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[3]/a\").click()\n",
    "                time.sleep(0.1)\n",
    "                browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[3]/a\").click()\n",
    "                time.sleep(1)\n",
    "\n",
    "            #soup_affaires3 = BeautifulSoup(browser.page_source)\n",
    "            for c in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[2]\"):\n",
    "            #for c in soup_affaires3.find_all('div',{'class':'tab-content cdtl'}):\n",
    "                try:\n",
    "                    globals()['liste_affaire'+cases[66:]+\"_\"+today2].append(str(c.text))\n",
    "                    df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                    df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "                    time.sleep(0.1)\n",
    "                    if len(globals()['liste_affaire'+cases[66:]+\"_\"+today2])< 3 or (str(globals()['liste_affaire'+cases[66:]+\"_\"+today2][2]) != 'No References Available' and (len(globals()['liste_affaire'+cases[66:]+\"_\"+today2])< 2 or len(globals()['liste_affaire'+cases[66:]+\"_\"+today2][2])<50 or globals()['liste_affaire'+cases[66:]+\"_\"+today2][2]=='nan' or globals()['liste_affaire'+cases[66:]+\"_\"+today2][2]=='')):\n",
    "                        #print(globals()['liste_affaire'+cases[66:]+\"_\"+today2][2])\n",
    "                        print(\"\\nL'onglet 'Procedural Details' est anormalement vide, le processus recommence le scrap dans 30 secondes\\n\")\n",
    "                        browser.quit()\n",
    "                        try: \n",
    "                            browser.close()\n",
    "                        except :\n",
    "                            pass\n",
    "                        time.sleep(30)\n",
    "                        browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                        browser.implicitly_wait(30)\n",
    "                        browser.get(str(cases))\n",
    "                        time.sleep(0.1)\n",
    "                        browser.get(str(cases))\n",
    "                        time.sleep(2)\n",
    "                        browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[3]/a\").click()\n",
    "                        time.sleep(0.1)\n",
    "                        browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[3]/a\").click()\n",
    "                        time.sleep(2)\n",
    "                        globals()['liste_affaire'+cases[66:]+\"_\"+today2]=globals()['liste_affaire'+cases[66:]+\"_\"+today2][0:2]\n",
    "                        for c in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[2]\"):\n",
    "            #for c in soup_affaires3.find_all('div',{'class':'tab-content cdtl'}):\n",
    "                            globals()['liste_affaire'+cases[66:]+\"_\"+today2].insert(2, str(c.text))\n",
    "                            df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                            df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "                            time.sleep(0.1)\n",
    "\n",
    "                except Exception as e: \n",
    "                    print(e)\n",
    "                    print('\\n!!!!!!!!! ATTENTION probleme détecté, le processus recommence!!!!!!!!! Attendre 5 minutes\\n')\n",
    "                    browser.quit()\n",
    "                    try: \n",
    "                        browser.close()\n",
    "                    except :\n",
    "                        pass\n",
    "                    time.sleep(300)\n",
    "                    browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                    browser.implicitly_wait(30)\n",
    "                    browser.get(str(cases))\n",
    "                    time.sleep(10)\n",
    "                    browser.get(str(cases))\n",
    "                    time.sleep(10)\n",
    "                    browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[3]/a\").click()\n",
    "                    time.sleep(0.1)\n",
    "                    browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[3]/a\").click()\n",
    "                    time.sleep(1)\n",
    "                    globals()['liste_affaire'+cases[66:]+\"_\"+today2]=globals()['liste_affaire'+cases[66:]+\"_\"+today2][0:2]\n",
    "                    for c in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[2]\"):\n",
    "                        globals()['liste_affaire'+cases[66:]+\"_\"+today2].append(str(c.text))\n",
    "                        df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                        df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "                        time.sleep(0.1)\n",
    "                        if len(globals()['liste_affaire'+cases[66:]+\"_\"+today2])<= 2 or (str(globals()['liste_affaire'+cases[66:]+\"_\"+today2][2]) != 'No References Available' and (len(globals()['liste_affaire'+cases[66:]+\"_\"+today2])< 2 or len(globals()['liste_affaire'+cases[66:]+\"_\"+today2][2])<50 or globals()['liste_affaire'+cases[66:]+\"_\"+today2][2]=='nan' or globals()['liste_affaire'+cases[66:]+\"_\"+today2][2]=='')):\n",
    "                            #print(globals()['liste_affaire'+cases[66:]+\"_\"+today2][2])\n",
    "                            print(\"\\nL'onglet 'Procedural Details' est anormalement vide, le processus recommence le scrap dans 30 secondes\\n\")\n",
    "                            browser.quit()\n",
    "                            try: \n",
    "                                browser.close()\n",
    "                            except :\n",
    "                                pass\n",
    "                            browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                            browser.implicitly_wait(30)\n",
    "                            time.sleep(30)\n",
    "                            browser.get(str(cases))\n",
    "                            time.sleep(0.1)\n",
    "                            browser.get(str(cases))\n",
    "                            time.sleep(2)\n",
    "                            browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[3]/a\").click()\n",
    "                            time.sleep(0.1)\n",
    "                            browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[3]/a\").click()\n",
    "                            time.sleep(2)\n",
    "                            globals()['liste_affaire'+cases[66:]+\"_\"+today2]=globals()['liste_affaire'+cases[66:]+\"_\"+today2][0:2]\n",
    "                            for c in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[2]\"):\n",
    "            #for c in soup_affaires3.find_all('div',{'class':'tab-content cdtl'}):\n",
    "                                globals()['liste_affaire'+cases[66:]+\"_\"+today2].append(str(c.text))\n",
    "                                df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                                df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "                                time.sleep(0.1)\n",
    "\n",
    "\n",
    "\n",
    "            browser.close()\n",
    "\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        print(\"\\nSCRAPING 2/3 REUSSI !\")\n",
    "\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        browser.quit()\n",
    "        try: \n",
    "            browser.close()\n",
    "        except :\n",
    "            pass\n",
    "\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        print(\"\\n Patienter 2 minutes\\n\")\n",
    "\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        for i in tqdm(range(100)):\n",
    "            time.sleep(1.20)\n",
    "\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        print('\\n    1.3- Scraping 3/3 : \\nSCRAPING DES AFFAIRES DE '+str((len(listenettoyee11)+len(listenettoyee12)))+' A '+str(len(listenettoyee2))+' : \\n')\n",
    "\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        options = Options()\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.headless = True\n",
    "        options.add_argument(\"--window-size=1920,1200\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        prefs = {\n",
    "            'profile.managed_default_content_settings.images': 2,\n",
    "            'download.prompt_for_download': False,\n",
    "            'download.directory_upgrade': True,\n",
    "            'safebrowsing.enabled': True\n",
    "        }\n",
    "        options.add_experimental_option('prefs', prefs)\n",
    "\n",
    "        i=len(listenettoyee12)+len(listenettoyee11)-1\n",
    "        for cases in tqdm(listenettoyee13):\n",
    "            i=i+1\n",
    "            try:\n",
    "\n",
    "            #requete_affaires = requests.get(cases)\n",
    "            #page_affaires = requete_affaires.content\n",
    "            #soup_affaires = BeautifulSoup(page_affaires)\n",
    "\n",
    "                browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                globals()['liste_affaire'+cases[66:]+\"_\"+today2]=[]\n",
    "                browser.implicitly_wait(30)\n",
    "            #browser.execute_script(\"window.open('');\")\n",
    "            #Window_List = browser.window_handles\n",
    "            #browser.switch_to_window(browser.window_handles[0])\n",
    "                browser.get(str(cases))\n",
    "                time.sleep(0.1)\n",
    "                browser.get(str(cases))\n",
    "                time.sleep(0.1)\n",
    "\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "                print('\\n!!!!!!!!! ATTENTION probleme détecté, le processus recommence!!!!!!!!! Attendre 5 minutes\\n')\n",
    "                browser.quit()\n",
    "                try: \n",
    "                    browser.close()\n",
    "                except :\n",
    "                    pass\n",
    "\n",
    "                time.sleep(300)\n",
    "                browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                browser.implicitly_wait(30)\n",
    "                browser.get(str(cases))\n",
    "                time.sleep(10)\n",
    "                browser.get(str(cases))\n",
    "                time.sleep(10)\n",
    "\n",
    "\n",
    "            #browser.find_element(by.linkText(str(cases))).sendKeys(Keys.chord(Keys.CONTROL,\"t\"))\n",
    "            #time.sleep (2.5)\n",
    "            #if i==200 or i==400 or i== 600 or i== 800:\n",
    "             #   print(\"sleep\")\n",
    "              #  time.sleep (60)\n",
    "            #Scrapping du titre :\n",
    "            for titre in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[1]/span\"):\n",
    "                try:\n",
    "                    globals()['liste_affaire'+cases[66:]+\"_\"+today2].append(str(titre.text))\n",
    "                    df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                    df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "                    if len(globals()['liste_affaire'+cases[66:]+\"_\"+today2])==0 or len(globals()['liste_affaire'+cases[66:]+\"_\"+today2][0])<25 or globals()['liste_affaire'+cases[66:]+\"_\"+today2][0]=='nan' or globals()['liste_affaire'+cases[66:]+\"_\"+today2][0]=='':\n",
    "                        print(\"ATTTENTION Le titre n'a pas été scrapé !!!, le processus recommence dans 30 secondes\")\n",
    "                        #print(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                        browser.quit()\n",
    "                        try: \n",
    "                            browser.close()\n",
    "                        except :\n",
    "                            pass\n",
    "                        time.sleep(30)\n",
    "                        browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                        browser.implicitly_wait(30)\n",
    "                        browser.get(str(cases))\n",
    "                        time.sleep(0.1)\n",
    "                        browser.get(str(cases))\n",
    "                        time.sleep(10)\n",
    "                        globals()['liste_affaire'+cases[66:]+\"_\"+today2]=[]\n",
    "                        for titre in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[1]/span\"):\n",
    "                            globals()['liste_affaire'+cases[66:]+\"_\"+today2].append(str(titre.text))\n",
    "                            df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                            df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "\n",
    "                except Exception as e: \n",
    "                    print(e)\n",
    "                    print('\\n!!!!!!!!! ATTENTION probleme détecté, le processus recommence!!!!!!!!! Attendre 5 minutes\\n')\n",
    "                    browser.quit()\n",
    "                    try: \n",
    "                        browser.close()\n",
    "                    except :\n",
    "                        pass\n",
    "\n",
    "                    time.sleep(300)\n",
    "                    browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                    browser.implicitly_wait(30)\n",
    "                    browser.get(str(cases))\n",
    "                    time.sleep(10)\n",
    "                    browser.get(str(cases))\n",
    "                    time.sleep(10)\n",
    "                    globals()['liste_affaire'+cases[66:]+\"_\"+today2]=[]\n",
    "                    for titre in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[1]/span\"):\n",
    "                        globals()['liste_affaire'+cases[66:]+\"_\"+today2].append(str(titre.text))\n",
    "                        df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                        df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "                        if len(globals()['liste_affaire'+cases[66:]+\"_\"+today2])==0 or len(globals()['liste_affaire'+cases[66:]+\"_\"+today2][0])<25 or globals()['liste_affaire'+cases[66:]+\"_\"+today2][0]=='nan' or globals()['liste_affaire'+cases[66:]+\"_\"+today2][0]=='':\n",
    "                            print(\"ATTTENTION Le titre n'a pas été scrapé !!!, le processus recommence dans 30 secondes\")\n",
    "                            #print(globals()['liste_affaire'+cases[66:]+\"_\"+today2][0])\n",
    "                            browser.quit()\n",
    "                            try: \n",
    "                                browser.close()\n",
    "                            except :\n",
    "                                pass\n",
    "                            time.sleep(30)\n",
    "                            browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                            browser.implicitly_wait(30)\n",
    "                            browser.get(str(cases))\n",
    "                            time.sleep(0.1)\n",
    "                            browser.get(str(cases))\n",
    "                            time.sleep(10)\n",
    "                            globals()['liste_affaire'+cases[66:]+\"_\"+today2]=[]\n",
    "                            for titre in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[1]/span\"):\n",
    "                                globals()['liste_affaire'+cases[66:]+\"_\"+today2].append(str(titre.text))\n",
    "                                df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                                df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "\n",
    "            #Scraping de l'onglet principal : \n",
    "\n",
    "            for a in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[2]\"):\n",
    "            #liste.append(str(a.get_attribute('href')))\n",
    "\n",
    "           # soup_affaires = BeautifulSoup(browser.page_source)\n",
    "            #for a in soup_affaires.find_all('div',{'class':'article article-body'}):\n",
    "                try:\n",
    "                    globals()['liste_affaire'+cases[66:]+\"_\"+today2].append(str(a.text))\n",
    "                    df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                    df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "\n",
    "                    if len(globals()['liste_affaire'+cases[66:]+\"_\"+today2])<=1 or len(globals()['liste_affaire'+cases[66:]+\"_\"+today2][1])<50 or globals()['liste_affaire'+cases[66:]+\"_\"+today2][1]=='nan' or globals()['liste_affaire'+cases[66:]+\"_\"+today2][1]=='':\n",
    "                        print(\"\\nATTTENTION L'onglet principal n'a pas été scrapé !!!, le processus recommence dans 30 secondes\\n\")\n",
    "                        #print(globals()['liste_affaire'+cases[66:]+\"_\"+today2][1])\n",
    "                        globals()['liste_affaire'+cases[66:]+\"_\"+today2]=globals()['liste_affaire'+cases[66:]+\"_\"+today2][0]\n",
    "                        browser.quit()\n",
    "                        try: \n",
    "                            browser.close()\n",
    "                        except :\n",
    "                            pass\n",
    "                        time.sleep(30)\n",
    "                        browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                        browser.implicitly_wait(30)\n",
    "                        browser.get(str(cases))\n",
    "                        time.sleep(0.1)\n",
    "                        browser.get(str(cases))\n",
    "                        time.sleep(2)\n",
    "                        for a in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[2]\"):\n",
    "                            globals()['liste_affaire'+cases[66:]+\"_\"+today2].insert(1, str(a.text))\n",
    "                            df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                            df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "                except Exception as e: \n",
    "                    print(e)\n",
    "                    print('\\n!!!!!!!!! ATTENTION probleme détecté, le processus recommence!!!!!!!!! Attendre 5 minutes\\n')\n",
    "                    browser.quit()\n",
    "                    try: \n",
    "                        browser.close()\n",
    "                    except :\n",
    "                        pass\n",
    "\n",
    "                    time.sleep(300)\n",
    "                    browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                    browser.implicitly_wait(30)\n",
    "                    browser.get(str(cases))\n",
    "                    time.sleep(10)\n",
    "                    browser.get(str(cases))\n",
    "                    time.sleep(10)\n",
    "                    globals()['liste_affaire'+cases[66:]+\"_\"+today2]=globals()['liste_affaire'+cases[66:]+\"_\"+today2][0]\n",
    "                    for a in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[2]\"):\n",
    "            #liste.append(str(a.get_attribute('href')))\n",
    "\n",
    "           # soup_affaires = BeautifulSoup(browser.page_source)\n",
    "            #for a in soup_affaires.find_all('div',{'class':'article article-body'}):\n",
    "                        globals()['liste_affaire'+cases[66:]+\"_\"+today2].insert(1, str(a.text))\n",
    "                        df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                        df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "\n",
    "                    if len(globals()['liste_affaire'+cases[66:]+\"_\"+today2])<=1 or len(globals()['liste_affaire'+cases[66:]+\"_\"+today2][1])<50 or globals()['liste_affaire'+cases[66:]+\"_\"+today2][1]=='nan' or globals()['liste_affaire'+cases[66:]+\"_\"+today2][1]=='':\n",
    "                        print(\"\\nATTTENTION L'onglet principal n'a pas été scrapé !!!, le processus recommence dans 30 secondes\\n\")\n",
    "                        #print(globals()['liste_affaire'+cases[66:]+\"_\"+today2][1])\n",
    "                        browser.quit()\n",
    "                        try: \n",
    "                            browser.close()\n",
    "                        except :\n",
    "                            pass\n",
    "                        globals()['liste_affaire'+cases[66:]+\"_\"+today2]=globals()['liste_affaire'+cases[66:]+\"_\"+today2][0]\n",
    "                        browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                        browser.implicitly_wait(30)\n",
    "                        time.sleep(30)\n",
    "                        browser.get(str(cases))\n",
    "                        time.sleep(0.1)\n",
    "                        browser.get(str(cases))\n",
    "                        time.sleep(2)\n",
    "                        for a in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[2]\"):\n",
    "                            globals()['liste_affaire'+cases[66:]+\"_\"+today2].insert(1, str(a.text))\n",
    "                            df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                            df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "\n",
    "           # Scraping de l'onglet Materials :\n",
    "           # browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[2]/a\").click()\n",
    "           # browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[2]/a\").click()\n",
    "\n",
    "\n",
    "           # time.sleep(1)\n",
    "           # soup_affaires2 = BeautifulSoup(browser.page_source)\n",
    "           # for b in soup_affaires2.find_all('div',{'class':'tab-content cdtl'}):\n",
    "           #   globals()['liste_affaire'+cases[66:]+\"_\"+today2].append(str(b))\n",
    "           #   df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "           #   df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "\n",
    "\n",
    "            #Scraping de l'onglet Procedural Details :\n",
    "            try:\n",
    "                browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[3]/a\").click()\n",
    "                time.sleep(0.1)\n",
    "                browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[3]/a\").click()\n",
    "                time.sleep(1)\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "                print('\\n!!!!!!!!! ATTENTION probleme détecté, le processus recommence!!!!!!!!! Attendre 5 minutes\\n')\n",
    "                browser.quit()\n",
    "                try: \n",
    "                    browser.close()\n",
    "                except :\n",
    "                    pass\n",
    "                time.sleep(300)\n",
    "                browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                browser.implicitly_wait(30)\n",
    "                browser.get(str(cases))\n",
    "                time.sleep(10)\n",
    "                browser.get(str(cases))\n",
    "                time.sleep(10)\n",
    "                browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[3]/a\").click()\n",
    "                time.sleep(0.1)\n",
    "                browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[3]/a\").click()\n",
    "                time.sleep(1)\n",
    "\n",
    "            #soup_affaires3 = BeautifulSoup(browser.page_source)\n",
    "            for c in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[2]\"):\n",
    "            #for c in soup_affaires3.find_all('div',{'class':'tab-content cdtl'}):\n",
    "                try:\n",
    "                    globals()['liste_affaire'+cases[66:]+\"_\"+today2].append(str(c.text))\n",
    "                    df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                    df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "                    time.sleep(0.1)\n",
    "                    if len(globals()['liste_affaire'+cases[66:]+\"_\"+today2])< 3 or (str(globals()['liste_affaire'+cases[66:]+\"_\"+today2][2]) != 'No References Available' and (len(globals()['liste_affaire'+cases[66:]+\"_\"+today2])< 2 or len(globals()['liste_affaire'+cases[66:]+\"_\"+today2][2])<50 or globals()['liste_affaire'+cases[66:]+\"_\"+today2][2]=='nan' or globals()['liste_affaire'+cases[66:]+\"_\"+today2][2]=='')):\n",
    "                        #print(globals()['liste_affaire'+cases[66:]+\"_\"+today2][2])\n",
    "                        print(\"\\nL'onglet 'Procedural Details' est anormalement vide, le processus recommence le scrap dans 30 secondes\\n\")\n",
    "                        browser.quit()\n",
    "                        try: \n",
    "                            browser.close()\n",
    "                        except :\n",
    "                            pass\n",
    "                        time.sleep(30)\n",
    "                        browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                        browser.implicitly_wait(30)\n",
    "                        browser.get(str(cases))\n",
    "                        time.sleep(0.1)\n",
    "                        browser.get(str(cases))\n",
    "                        time.sleep(2)\n",
    "                        browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[3]/a\").click()\n",
    "                        time.sleep(0.1)\n",
    "                        browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[3]/a\").click()\n",
    "                        time.sleep(2)\n",
    "                        globals()['liste_affaire'+cases[66:]+\"_\"+today2]=globals()['liste_affaire'+cases[66:]+\"_\"+today2][0:2]\n",
    "                        for c in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[2]\"):\n",
    "            #for c in soup_affaires3.find_all('div',{'class':'tab-content cdtl'}):\n",
    "                            globals()['liste_affaire'+cases[66:]+\"_\"+today2].insert(2, str(c.text))\n",
    "                            df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                            df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "                            time.sleep(0.1)\n",
    "\n",
    "                except Exception as e: \n",
    "                    print(e)\n",
    "                    print('\\n!!!!!!!!! ATTENTION probleme détecté, le processus recommence!!!!!!!!! Attendre 5 minutes\\n')\n",
    "                    browser.quit()\n",
    "                    try: \n",
    "                        browser.close()\n",
    "                    except :\n",
    "                        pass\n",
    "                    time.sleep(300)\n",
    "                    browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                    browser.implicitly_wait(30)\n",
    "                    browser.get(str(cases))\n",
    "                    time.sleep(10)\n",
    "                    browser.get(str(cases))\n",
    "                    time.sleep(10)\n",
    "                    browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[3]/a\").click()\n",
    "                    time.sleep(0.1)\n",
    "                    browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[3]/a\").click()\n",
    "                    time.sleep(1)\n",
    "                    globals()['liste_affaire'+cases[66:]+\"_\"+today2]=globals()['liste_affaire'+cases[66:]+\"_\"+today2][0:2]\n",
    "                    for c in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[2]\"):\n",
    "                        globals()['liste_affaire'+cases[66:]+\"_\"+today2].append(str(c.text))\n",
    "                        df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                        df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "                        time.sleep(0.1)\n",
    "                        if len(globals()['liste_affaire'+cases[66:]+\"_\"+today2])<= 2 or (str(globals()['liste_affaire'+cases[66:]+\"_\"+today2][2]) != 'No References Available' and (len(globals()['liste_affaire'+cases[66:]+\"_\"+today2])< 2 or len(globals()['liste_affaire'+cases[66:]+\"_\"+today2][2])<50 or globals()['liste_affaire'+cases[66:]+\"_\"+today2][2]=='nan' or globals()['liste_affaire'+cases[66:]+\"_\"+today2][2]=='')):\n",
    "                            #print(globals()['liste_affaire'+cases[66:]+\"_\"+today2][2])\n",
    "                            print(\"\\nL'onglet 'Procedural Details' est anormalement vide, le processus recommence le scrap dans 30 secondes\\n\")\n",
    "                            browser.quit()\n",
    "                            try: \n",
    "                                browser.close()\n",
    "                            except :\n",
    "                                pass\n",
    "                            browser = webdriver.Chrome(options=options,executable_path=emplacement_driver_chrome)\n",
    "                            browser.implicitly_wait(30)\n",
    "                            time.sleep(30)\n",
    "                            browser.get(str(cases))\n",
    "                            time.sleep(0.1)\n",
    "                            browser.get(str(cases))\n",
    "                            time.sleep(2)\n",
    "                            browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[3]/a\").click()\n",
    "                            time.sleep(0.1)\n",
    "                            browser.find_element_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[3]/a\").click()\n",
    "                            time.sleep(2)\n",
    "                            globals()['liste_affaire'+cases[66:]+\"_\"+today2]=globals()['liste_affaire'+cases[66:]+\"_\"+today2][0:2]\n",
    "                            for c in browser.find_elements_by_xpath(\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/div[2]\"):\n",
    "            #for c in soup_affaires3.find_all('div',{'class':'tab-content cdtl'}):\n",
    "                                globals()['liste_affaire'+cases[66:]+\"_\"+today2].append(str(c.text))\n",
    "                                df = pd.DataFrame(globals()['liste_affaire'+cases[66:]+\"_\"+today2])\n",
    "                                df.to_csv(emplacement_fichier_affaires+'/Affaire'+listenettoyee2[i][66:]+\"_\"+today2+'.csv', index=False)\n",
    "                                time.sleep(0.1)\n",
    "\n",
    "\n",
    "\n",
    "            browser.close()\n",
    "\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        print(\"\\nSCRAPING 3/3 REUSSI !\")\n",
    "\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        browser.quit()\n",
    "        try: \n",
    "            browser.close()\n",
    "        except :\n",
    "            pass\n",
    "\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        print(\"\\nPatienter 10 secondes\\n\")\n",
    "\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        for i in tqdm(range(100)):\n",
    "            time.sleep(0.10)\n",
    "\n",
    "\n",
    "        # b) Comparaison des affaires\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        print(\"\\n  2- Comparaison des affaires :\\n\")\n",
    "\n",
    "\n",
    "        # In[105]:\n",
    "\n",
    "\n",
    "        import os\n",
    "        dossier_affaires= os.listdir(emplacement_fichier_affaires)\n",
    "        listetecsvaffaires=[]\n",
    "        for csv in dossier_affaires:\n",
    "          listetecsvaffaires.append(csv)\n",
    "\n",
    "\n",
    "        # In[106]:\n",
    "\n",
    "\n",
    "        #len(listetecsvaffaires)\n",
    "\n",
    "\n",
    "        # In[107]:\n",
    "\n",
    "\n",
    "        #len(listenettoyee2)\n",
    "\n",
    "\n",
    "        # In[108]:\n",
    "\n",
    "\n",
    "        listetecsvaffaires.sort()\n",
    "\n",
    "\n",
    "        # In[109]:\n",
    "\n",
    "\n",
    "        def listecsv_affaires_debut(x):\n",
    "          chaine = str(x)\n",
    "          pos1 = chaine.find('Affaire')\n",
    "          pos2 = -19\n",
    "          sousChaine = chaine[pos1:pos2]\n",
    "          #print (sousChaine)\n",
    "          return(sousChaine)\n",
    "\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        print('    2.1- Classement des affaires :\\n')\n",
    "\n",
    "\n",
    "        # In[110]:\n",
    "\n",
    "\n",
    "        liste_affaire_double=[]\n",
    "        for affaire1 in tqdm(listetecsvaffaires):\n",
    "          i=0    \n",
    "          for affaire2 in listetecsvaffaires:\n",
    "\n",
    "            if listecsv_affaires_debut(listetecsvaffaires[listetecsvaffaires.index(affaire1)])==listecsv_affaires_debut(listetecsvaffaires[listetecsvaffaires.index(affaire2)]):\n",
    "              i=i+1\n",
    "              if i>1:\n",
    "                liste_affaire_double.append(affaire1)\n",
    "                liste_affaire_double.append(affaire2)\n",
    "\n",
    "\n",
    "        # In[119]:\n",
    "\n",
    "\n",
    "        liste_affaire_double=list(set(liste_affaire_double))\n",
    "\n",
    "\n",
    "        # In[120]:\n",
    "\n",
    "\n",
    "        liste_affaire_double.sort(reverse=True)\n",
    "\n",
    "\n",
    "        # In[121]:\n",
    "\n",
    "\n",
    "        #listecsv_affaires_debut(liste_affaire_double[6])\n",
    "\n",
    "\n",
    "        # In[122]:\n",
    "\n",
    "\n",
    "        i=0\n",
    "        for affaire_en_double in liste_affaire_double:\n",
    "          i=i+1\n",
    "\n",
    "          globals()['liste_double'+str(i)]=[]\n",
    "          for affaire_en_double2 in liste_affaire_double:\n",
    "            if affaire_en_double[0:-19]==affaire_en_double2[0:-19]:\n",
    "              globals()['liste_double'+str(i)].append(affaire_en_double2)\n",
    "\n",
    "\n",
    "        # In[123]:\n",
    "\n",
    "\n",
    "        #liste_double1\n",
    "\n",
    "\n",
    "        # In[124]:\n",
    "\n",
    "\n",
    "        def Diff(li1, li2): \n",
    "            li_dif = [i for i in li1 + li2 if i not in li1 or i not in li2] \n",
    "            return li_dif \n",
    "\n",
    "\n",
    "        # In[125]:\n",
    "\n",
    "\n",
    "        #liste_affaire_double\n",
    "\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        print(\"\\n  3- Création du message de notification s'il y a des modifications dans les affaires :\\n\")\n",
    "\n",
    "\n",
    "        # In[126]:\n",
    "\n",
    "\n",
    "        today = str(datetime.now(pytz.timezone(\"Europe/Paris\")).strftime(\"%d/%m/%Y à %H:%M:%S\"))\n",
    "        k=0\n",
    "        message2=[]\n",
    "        for j in tqdm(range(1,(len(liste_affaire_double)+1))):\n",
    "          dernier_csv=pd.read_csv(emplacement_fichier_affaires+'/'+globals()['liste_double'+str(j)][0])\n",
    "          avant_dernier_csv=pd.read_csv(emplacement_fichier_affaires+'/'+globals()['liste_double'+str(j)][1])\n",
    "          avant_dernier_csvliste_affaire=avant_dernier_csv.values.tolist()\n",
    "          dernier_csvliste_affaire=dernier_csv.values.tolist()\n",
    "          modificationsliste=Diff(avant_dernier_csvliste_affaire,dernier_csvliste_affaire)\n",
    "\n",
    "          if avant_dernier_csv.equals(dernier_csv)==False:\n",
    "              k=k+1      \n",
    "              nom_affaire=str(dernier_csvliste_affaire[0])\n",
    "              chaine=str(globals()['liste_double'+str(j)][0])\n",
    "              pos1 = len('Affaire')+chaine.find('Affaire')\n",
    "              pos2 = chaine.find('_')\n",
    "              print (\"\\nModifications apportées à l'affaire : \"+globals()['liste_double'+str(j)][0][pos1:-20])\n",
    "              #for y in modificationsliste :\n",
    "                #modificationsliste = [x for x in y if str(x) != 'nan']\n",
    "              modificationsliste=['\\n'.join(sub_list) for sub_list in modificationsliste]\n",
    "              #print(modificationsliste)\n",
    "              ancien =modificationsliste[0:int((len(modificationsliste)/2))]\n",
    "              nouveau=modificationsliste[int((len(modificationsliste)/2)):]\n",
    "              #liste_mail=[]\n",
    "              #for i in range(0,len(modificationsliste)):\n",
    "               # liste_mail.append(adresse_modification(modificationsliste[i]))\n",
    "              #sauvegarde message2\n",
    "              #  i=i+1\n",
    "              time.sleep(2)\n",
    "\n",
    "            #recuperer les phrases qui different entre ancien et nouveau\n",
    "              ancien2=''\n",
    "              for l in range(0,len(ancien)):\n",
    "                  ancien2=ancien2+ancien[l]\n",
    "              indexancien=([(n.start(0)) for n in re.finditer('\\n', ancien2)])\n",
    "              indexancien.insert(0,0)\n",
    "              indexancien.append(len(ancien2))\n",
    "\n",
    "              nouveau2=''\n",
    "              for m in range(0,len(nouveau)):\n",
    "                  nouveau2=nouveau2+nouveau[m]\n",
    "              indexnouveau=([(o.start(0)) for o in re.finditer('\\n', nouveau2)])\n",
    "              indexnouveau.insert(0,0)\n",
    "              indexnouveau.append(len(nouveau2))\n",
    "\n",
    "              listeancien=[]\n",
    "              p=0\n",
    "              for q in range(0,len(indexancien)-1):\n",
    "                  p=p+1\n",
    "                  listeancien.append(ancien2[indexancien[q]:indexancien[p]])\n",
    "\n",
    "              listenouveau=[]\n",
    "              r=0\n",
    "              for s in range(0,len(indexnouveau)-1):\n",
    "                  r=r+1\n",
    "                  listenouveau.append(nouveau2[indexnouveau[s]:indexnouveau[r]])\n",
    "\n",
    "              message_Diff=Diff(listeancien,listenouveau) \n",
    "\n",
    "              #ancienneversion=message_Diff[0:int((len(message_Diff)/2))]\n",
    "              #nouvelleversion=message_Diff[int((len(message_Diff)/2)):]\n",
    "\n",
    "            #Création du message\n",
    "              globals()['liste_double'+str(j)][0]=globals()['liste_double'+str(j)][0].replace('_','/')\n",
    "\n",
    "              message2.append(\"* Modifications apportées à l'affaire : \"+globals()['liste_double'+str(j)][0][pos1:-20] +\"\\n- Lien : https://icsid.worldbank.org/en/Pages/cases/casedetail.aspx?CaseNo=\"+globals()['liste_double'+str(j)][0][pos1:-20]+ \" \\n- Nom de l'affaire : \"+str(nom_affaire)+\" \\n\\nVoici ce qui a été modifié, ajouté ou supprimé : \"+str(message_Diff)+' \\nNotification créée le '+today)\n",
    "\n",
    "              print('Modifications sauvegardées dans la liste message2\\n')\n",
    "\n",
    "        if k==0 :\n",
    "          print(' \\nPas de modification dans les affaires, pas de notification créée\\n')\n",
    "          message2=[]\n",
    "\n",
    "\n",
    "\n",
    "        # In[127]:\n",
    "\n",
    "\n",
    "        message2=list(set(message2))\n",
    "\n",
    "\n",
    "        # c) Nettoyage\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        print(\"\\n  4- Nettoyage du message de notification :\\n\")\n",
    "\n",
    "\n",
    "        # In[128]:\n",
    "\n",
    "\n",
    "        for i in range(0, len(message2)):\n",
    "            message2[i]=message2[i].replace('[','\\n')\n",
    "            message2[i]=message2[i].replace(']','')\n",
    "            message2[i]=message2[i].replace(\"'Subject of Dispute:\", \"\\nONGLET PRINCIPAL :\\nSubject of Dispute:\")\n",
    "            message2[i]=message2[i].replace(\"', '(a) Original Proceeding\\\\nDate\\\\nDevelopment\", \"\\n\\nONGLET PROCEDURAL DETAILS :\")\n",
    "            message2[i]=message2[i].replace('\\\\n','\\n')\n",
    "            message2[i]=message2[i].replace(\"', '\",'\\n')\n",
    "\n",
    "\n",
    "        # In[129]:\n",
    "\n",
    "\n",
    "        #message2\n",
    "\n",
    "\n",
    "        # In[130]:\n",
    "\n",
    "\n",
    "        browser.quit()\n",
    "        try: \n",
    "            browser.close()\n",
    "        except :\n",
    "            pass\n",
    "\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        print(\" Patienter 10 secondes\\n\")\n",
    "\n",
    "\n",
    "        # In[131]:\n",
    "\n",
    "\n",
    "        for i in tqdm(range(100)):\n",
    "            time.sleep(0.10)\n",
    "\n",
    "\n",
    "        # III. Nettoyage des dossiers\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        print('\\nIII. NETTOYAGE DU DOSSIER : '+emplacement_fichier_affaires)\n",
    "\n",
    "\n",
    "        # In[99]:\n",
    "\n",
    "\n",
    "        import os\n",
    "        dossier_affaires= os.listdir(emplacement_fichier_affaires)\n",
    "\n",
    "        for csv in dossier_affaires:\n",
    "          if csv[-19:-4]!= today2:\n",
    "            #os.rename(emplacement_fichier_affaires+\"/\"+str(csv), emplacement_ancien_csv+\"/\"+str(csv))\n",
    "            shutil.move(emplacement_fichier_affaires+\"/\"+str(csv), emplacement_ancien_csv+\"/\"+str(csv))\n",
    "            #os.replace(emplacement_fichier_affaires+\"/\"+str(csv), emplacement_ancien_csv+\"/\"+str(csv))\n",
    "        print(\"\\nAnciens CSV déplacés vers le dossier : \"+emplacement_ancien_csv)\n",
    "\n",
    "\n",
    "        # IV. Envoi du message\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        print('\\nIV. ENVOI DU MESSAGE SI DES MODIFICATIONS ONT ETE APPORTEES :\\n')\n",
    "\n",
    "\n",
    "        # In[132]:\n",
    "\n",
    "\n",
    "        message3=message1+message2\n",
    "\n",
    "\n",
    "        # In[133]:\n",
    "\n",
    "\n",
    "        def send_mail(message):\n",
    "          pw=password\n",
    "          pw\n",
    "          adresse=adresse_envoi\n",
    "          adresse\n",
    "          login\n",
    "          #destinataire=destinataire\n",
    "          destinataire\n",
    "          sujet=\"Modification(s) apportée(s) à la page ICSID.wordbank.org\"\n",
    "          sujet\n",
    "          message=str(message)\n",
    "          message\n",
    "          msg = MIMEMultipart()\n",
    "          msg['From'] = adresse\n",
    "          msg['To'] = destinataire\n",
    "          msg['Subject'] = sujet \n",
    "          message = message\n",
    "          msg.attach(MIMEText(message))\n",
    "          mailserver = smtplib.SMTP(smtp, port)\n",
    "          mailserver.ehlo()\n",
    "          mailserver.starttls() #activer si besoin\n",
    "          #mailserver.ehlo() #activer si besoin\n",
    "          mailserver.login(login, pw)\n",
    "          #mailserver.set_debuglevel(1) #activer si besoin\n",
    "          mailserver.sendmail(adresse,destinataire, msg.as_string())\n",
    "          print(\"\\n Message envoyé !\")\n",
    "          mailserver.quit()\n",
    "\n",
    "\n",
    "        # In[134]:\n",
    "\n",
    "\n",
    "        message1string=\"\"\n",
    "        for i in range(0,len(message1)):\n",
    "          message1string=message1string+'\\n\\n'+str(message1[i])\n",
    "\n",
    "        message2string=\"\"\n",
    "        for i in range(0,len(message2)):\n",
    "          message2string=message2string+'\\n\\n'+str(message2[i])\n",
    "\n",
    "\n",
    "        # In[135]:\n",
    "\n",
    "\n",
    "        if len(message3)>0:\n",
    "\n",
    "          send_mail(message1string+\"\\n\\n********************\"+message2string)\n",
    "        else:\n",
    "          print('\\nAucune modification de la page, aucun mail envoyé !\\n')\n",
    "\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        #message2string\n",
    "\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        today= str(datetime.now(pytz.timezone(\"Europe/Paris\")).strftime(\"%H:%M:%S LE %d/%m/%Y\"))\n",
    "        print('\\nFIN DU PROGRAMME A '+today)\n",
    "\n",
    "\n",
    "        # In[ ]:\n",
    "\n",
    "\n",
    "        #Erreurs rencontrées lors de la conception du code :\n",
    "\n",
    "\n",
    "            #NoSuchElementException: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"/html/body/form/div[12]/div/div/div[4]/div[2]/div[2]/div[1]/div/div[4]/div[2]/div/div[1]/div/div/div/div[1]/div/ul/li[2]/a\"}\n",
    "         #(Session info: chrome=77.0.3865.90)\n",
    "\n",
    "            #OSError: [Errno 12] Cannot allocate memory\n",
    "\n",
    "            #ElementClickInterceptedException: Message: element click intercepted: Element <a class=\"tabsctionCclss\" data-toggle=\"tab\" href=\"#sectionc\" ng-click=\"getproceduredetails()\" aria-expanded=\"true\">...</a> is not clickable at point (840, 272). Other element would receive the click: <div class=\"casedetltile\">...</div>\n",
    "         #(Session info: headless chrome=77.0.3865.90)\n",
    "\n",
    "            #TimeoutException: Message: timeout\n",
    "         #(Session info: headless chrome=77.0.3865.90)\n",
    "\n",
    "            #MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=49215): Max retries exceeded with url: /session/dd250767f8bc7c7d215a3897e62249b7/window (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7facdb1a9908>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
    "except Exception as e:\n",
    "    \n",
    "        from email.mime.multipart import MIMEMultipart\n",
    "        from email.mime.text import MIMEText\n",
    "        import smtplib\n",
    "\n",
    "\n",
    "\n",
    "        def send_mail(message):\n",
    "            pw=password\n",
    "            pw\n",
    "            adresse=adresse_envoi\n",
    "            adresse\n",
    "            login\n",
    "            #destinataire=destinataire\n",
    "            destinataire\n",
    "            sujet=\"La page https://icsid.worldbank.org n'a pas pu être scrapée, il y a une erreur !\"\n",
    "            sujet\n",
    "            message=str(message)\n",
    "            message\n",
    "            msg = MIMEMultipart()\n",
    "            msg['From'] = adresse\n",
    "            msg['To'] = destinataire\n",
    "            msg['Subject'] = sujet \n",
    "            message = message\n",
    "            msg.attach(MIMEText(message))\n",
    "            mailserver = smtplib.SMTP(smtp, port)\n",
    "            mailserver.ehlo()\n",
    "            mailserver.starttls() #activer si besoin\n",
    "            #mailserver.ehlo() #activer si besoins\n",
    "\n",
    "            mailserver.login(login, pw)\n",
    "            mailserver.sendmail(adresse,destinataire, msg.as_string())\n",
    "            print(\" Message envoyé !\")\n",
    "            mailserver.quit()\n",
    "\n",
    "        send_mail('Erreur lors du scraping de la page ISCID. Veuillez verifier le programme \\n'+e)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
